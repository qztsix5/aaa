# baidu_search_agent.py - ä¼˜åŒ–ç‰ˆ
import requests
from bs4 import BeautifulSoup
import urllib.parse
import asyncio
import logging
from typing import List, Dict
import re
import time

logger = logging.getLogger(__name__)

class BaiduSearchAgent:
    """ä½¿ç”¨ç™¾åº¦æœç´¢å¼•æ“çš„ä»£ç†"""
    
    def __init__(self):
        self.base_url = "https://www.baidu.com/s"
        # æ›´æ–°User-Agent
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def search_baidu(self, query: str, num_results: int = 8) -> List[Dict]:
        """ä½¿ç”¨ç™¾åº¦æœç´¢å¹¶è§£æç»“æœ"""
        try:
            # ç¼–ç æŸ¥è¯¢å‚æ•°
            params = {
                "wd": query,
                "rn": num_results,  # ç»“æœæ•°é‡
                "ie": "utf-8",
                "cl": 3,  # ç½‘é¡µç±»å‹
            }
            
            logger.info(f"æœç´¢ç™¾åº¦: {query}")
            response = self.session.get(self.base_url, params=params, timeout=15)
            response.raise_for_status()
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            results = self._parse_baidu_results_optimized(soup)
            
            return results
            
        except Exception as e:
            logger.error(f"ç™¾åº¦æœç´¢å¤±è´¥: {e}")
            # è¿”å›æ¨¡æ‹Ÿæ•°æ®ä½œä¸ºå¤‡ç”¨
            return self._get_fallback_results(query)
    
    def _parse_baidu_results_optimized(self, soup: BeautifulSoup) -> List[Dict]:
        """ç™¾åº¦ç»“æœè§£æ"""
        results = []
        
        # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
        selectors = [
            'div.result',
            'div.c-container',
            'div[class*="result"]',
            'div[class*="c-container"]',
            'div.content-left',
            'div[srcid]'
        ]
        
        for selector in selectors:
            result_containers = soup.select(selector)
            if result_containers:
                logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(result_containers)} ä¸ªç»“æœ")
                for container in result_containers[:10]:
                    try:
                        result = self._parse_single_result(container)
                        if result and result["title"]:
                            results.append(result)
                    except Exception as e:
                        logger.debug(f"è§£æå•ä¸ªç»“æœå¤±è´¥: {e}")
                break  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„é€‰æ‹©å™¨
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç»“æœï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
        if not results:
            results = self._parse_backup_results(soup)
        
        # å»é‡
        seen_titles = set()
        unique_results = []
        for result in results:
            if result["title"] not in seen_titles:
                seen_titles.add(result["title"])
                unique_results.append(result)
        
        return unique_results[:8]  # é™åˆ¶æ•°é‡
    
    def _parse_single_result(self, container) -> Dict:
        """è§£æå•ä¸ªæœç´¢ç»“æœ"""
        # æå–æ ‡é¢˜
        title_elem = (container.find('h3') or 
                     container.find('a', class_=re.compile(r'title|head')) or
                     container.find('a'))
        
        if not title_elem:
            return None
            
        title = self._clean_text(title_elem.get_text())
        link = title_elem.get('href', '')
        
        # å¤„ç†ç™¾åº¦è·³è½¬é“¾æ¥
        if link.startswith('/'):
            link = "https://www.baidu.com" + link
        
        # ä¼˜åŒ–æ‘˜è¦æå– - å°è¯•å¤šç§é€‰æ‹©å™¨
        abstract = self._extract_abstract_optimized(container)
        
        # è¿‡æ»¤å¹¿å‘Š
        if self._is_ad(container):
            return None
        
        return {
            "title": title,
            "link": link,
            "abstract": abstract,
            "source": "ç™¾åº¦æœç´¢"
        }
    
    def _extract_abstract_optimized(self, container) -> str:
        """ä¼˜åŒ–æ‘˜è¦æå–"""
        # å°è¯•å¤šç§æ‘˜è¦é€‰æ‹©å™¨
        abstract_selectors = [
            'div.c-abstract',
            'div.content',
            'div.desc',
            'div.summary',
            'span.content-right',
            'div[class*="abstract"]',
            'div[class*="desc"]',
            'div[class*="summary"]'
        ]
        
        for selector in abstract_selectors:
            abstract_elem = container.select_one(selector)
            if abstract_elem:
                abstract_text = self._clean_text(abstract_elem.get_text())
                if abstract_text and len(abstract_text) > 10:
                    return abstract_text
        
        # å¦‚æœä¸Šè¿°é€‰æ‹©å™¨éƒ½å¤±è´¥ï¼Œå°è¯•ä»æ•´ä¸ªå®¹å™¨ä¸­æå–éæ ‡é¢˜æ–‡æœ¬
        container_text = self._clean_text(container.get_text())
        title_elem = container.find('h3') or container.find('a')
        if title_elem:
            title_text = self._clean_text(title_elem.get_text())
            # ä»å®Œæ•´æ–‡æœ¬ä¸­ç§»é™¤æ ‡é¢˜
            if title_text and title_text in container_text:
                abstract = container_text.replace(title_text, '').strip()
                if len(abstract) > 20:
                    return abstract
        
        return "æš‚æ— è¯¦ç»†æ‘˜è¦"
    
    def _parse_backup_results(self, soup: BeautifulSoup) -> List[Dict]:
        """å¤‡ç”¨è§£ææ–¹æ³•"""
        backup_results = []
        
        # å°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«é“¾æ¥çš„å®¹å™¨
        link_containers = soup.find_all(['div', 'section', 'article'], class_=True)
        
        for container in link_containers[:20]:
            try:
                link_elem = container.find('a', href=True)
                if not link_elem:
                    continue
                
                title = self._clean_text(link_elem.get_text())
                link = link_elem['href']
                
                if not title or len(title) < 5:
                    continue
                
                # ç®€å•è¿‡æ»¤å¹¿å‘Š
                if any(word in title.lower() for word in ['å¹¿å‘Š', 'æ¨å¹¿']):
                    continue
                
                # æå–å®¹å™¨å†…çš„æ–‡æœ¬ä½œä¸ºæ‘˜è¦
                container_text = self._clean_text(container.get_text())
                abstract = container_text.replace(title, '').strip()
                abstract = self._clean_abstract(abstract)
                
                backup_results.append({
                    "title": title,
                    "link": link,
                    "abstract": abstract if abstract else "æš‚æ— è¯¦ç»†æ‘˜è¦",
                    "source": "ç™¾åº¦æœç´¢(å¤‡ç”¨)"
                })
                    
            except Exception as e:
                continue
        
        return backup_results
    
    def _is_ad(self, container) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Š"""
        ad_indicators = ['å¹¿å‘Š', 'æ¨å¹¿', 'ad', 'advertisement']
        container_text = container.get_text().lower()
        return any(indicator in container_text for indicator in ad_indicators)
    
    def _clean_abstract(self, abstract: str) -> str:
        """æ¸…ç†æ‘˜è¦æ–‡æœ¬"""
        if not abstract:
            return ""
        
        # ç§»é™¤è¿‡çŸ­çš„å†…å®¹
        if len(abstract) < 10:
            return ""
        
        # ç§»é™¤å¸¸è§å™ªéŸ³
        noise_patterns = [
            r'ç™¾åº¦å¿«ç…§.*',
            r'ç›¸å…³è§†é¢‘.*',
            r'å¹¿å‘Š',
            r'æ¨å¹¿',
            r'æŸ¥çœ‹æ›´å¤š',
            r'\.\.\.',
        ]
        
        for pattern in noise_patterns:
            abstract = re.sub(pattern, '', abstract)
        
        # é™åˆ¶é•¿åº¦
        if len(abstract) > 200:
            abstract = abstract[:197] + "..."
        
        return abstract.strip()
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        # æ›¿æ¢å¤šä¸ªç©ºç™½å­—ç¬¦ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _get_fallback_results(self, query: str) -> List[Dict]:
        """è·å–å¤‡ç”¨ç»“æœï¼ˆå½“æœç´¢å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        return [
            {
                "title": f"å…³äº'{query}'çš„æœç´¢ç»“æœ",
                "link": "https://www.baidu.com",
                "abstract": f"ç”±äºç½‘ç»œæˆ–è§£æé—®é¢˜ï¼Œæ— æ³•è·å–'{query}'çš„å®æ—¶æœç´¢ç»“æœã€‚å»ºè®®ç›´æ¥è®¿é—®ç™¾åº¦æœç´¢æŸ¥çœ‹æœ€æ–°ä¿¡æ¯ã€‚",
                "source": "ç³»ç»Ÿæç¤º"
            }
        ]
    
    def format_search_results(self, results: List[Dict], query: str = "") -> str:
        """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
        if not results:
            return f"ğŸ”ğŸ” æœªæ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³ç»“æœ"
        
        # å¦‚æœæ˜¯å¤‡ç”¨ç»“æœï¼Œç‰¹æ®Šå¤„ç†
        if len(results) == 1 and results[0]["source"] == "ç³»ç»Ÿæç¤º":
            return f"ã€æœç´¢æç¤ºã€‘: {results[0]['abstract']}"
        
        formatted = f"ã€ç™¾åº¦æœç´¢: {query}ã€‘\n\n"
        
        for i, result in enumerate(results[:5], 1):
            formatted += f"{i}. ğŸ“° {result['title']}\n"
            formatted += f"   æ‘˜è¦: {result['abstract']}\n"
            formatted += f"   æ¥æº: {result['source']}\n\n"
        
        # æ·»åŠ è´¢åŠ¡æœç´¢ä¸“ç”¨æç¤º
        financial_keywords = ["è´¢åŠ¡", "è´¢æŠ¥", "æ”¶å…¥", "åˆ©æ¶¦", "å¹´æŠ¥", "å­£åº¦æŠ¥å‘Š"]
        if any(keyword in query for keyword in financial_keywords):
            formatted += "ğŸ’¡ğŸ’¡ è´¢åŠ¡ä¿¡æ¯æç¤º: ä»¥ä¸Šä¿¡æ¯æ¥è‡ªå…¬å¼€æœç´¢ï¼Œè¯·ä»¥å…¬å¸å®˜æ–¹å…¬å‘Šä¸ºå‡†"
        else:
            formatted += "ğŸ’¡ğŸ’¡ æç¤º: ä»¥ä¸Šä¿¡æ¯æ¥è‡ªç™¾åº¦æœç´¢ï¼Œè¯·è°¨æ…å‚è€ƒå…¶å‡†ç¡®æ€§"
        
        return formatted
    
    async def async_search(self, query: str) -> str:
        """å¼‚æ­¥æœç´¢æ¥å£"""
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(None, self.search_baidu, query)
        return self.format_search_results(results, query)

# åˆ›å»ºå…¨å±€å®ä¾‹
baidu_agent = BaiduSearchAgent()

# é€‚é…åŸæœ‰æ¥å£çš„å‡½æ•°
async def search_market_info(query: str) -> str:
    """é€‚é…åŸæœ‰ç³»ç»Ÿçš„æœç´¢å‡½æ•°"""
    return await baidu_agent.async_search(query)

# ä¸“é—¨ç”¨äºè´¢åŠ¡æœç´¢çš„å‡½æ•°
async def search_financial_info(company: str, year: str = "") -> str:
    """æœç´¢å…¬å¸è´¢åŠ¡ä¿¡æ¯"""
    search_query = f"{company} {year}å¹´ è´¢åŠ¡æŠ¥å‘Š å¹´æŠ¥" if year else f"{company} æœ€æ–°è´¢åŠ¡æ•°æ®"
    return await baidu_agent.async_search(search_query)
 